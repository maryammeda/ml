{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a625691",
   "metadata": {},
   "source": [
    "# Confusion Matrix - Sensitivity & Specificity\n",
    "\n",
    "YT video 1 - https://www.youtube.com/watch?v=Kdsp6soqA7o&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=3\n",
    "\n",
    "YT video 2 - https://www.youtube.com/watch?v=vP06aMoz4v8&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=4\n",
    "\n",
    "A confusion matrix is a table that shows how well a classification model performs by comparing predicted values with actual values. It displays four types of predictions:\n",
    "\n",
    "True Positive (TP): Correctly predicted positive \n",
    "\n",
    "True Negative (TN): Correctly predicted negative \n",
    "\n",
    "False Positive (FP): Incorrectly predicted positive\n",
    " \n",
    "False Negative (FN): Incorrectly predicted negative "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50a256",
   "metadata": {},
   "source": [
    "## Confusion Matrix Using Cross Validation - \n",
    "\n",
    "First, we split our data into Training and Testing sets. We use the Training data to build the model, and then we use the model to make predictions on the Testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3c8e6",
   "metadata": {},
   "source": [
    "### Create simple data - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fdb6166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create simple heart disease data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 100 people with random ages and cholesterol levels\n",
    "age = np.random.normal(55, 15, 100)  # Mean=55, SD=15, n=100\n",
    "cholesterol = np.random.normal(200, 50, 100)  # Mean=200, SD=50, n=100\n",
    "\n",
    "# Simple rule: high age + high cholesterol = heart disease\n",
    "risk = (age - 50) / 20 + (cholesterol - 200) / 100 # Create a simple rule to simulate risk  \n",
    "\n",
    "# astype(int) converts the risk values to integers (0 or 1), not booleans\n",
    "actual = (risk > 0.5).astype(int) # 1 = has heart disease, 0 = no heart disease\n",
    "\n",
    "# Combine age and cholesterol into a 2D feature array\n",
    "X = np.column_stack((age, cholesterol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794f524",
   "metadata": {},
   "source": [
    "If we have N classes/categories to predict, the confusion matrix will have N rows and N columns, creating an NÃ—N matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6fe61",
   "metadata": {},
   "source": [
    "### Train Models and Get Predictions -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a895ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation to get predictions (5-fold CV)\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=42)  # Use 10 trees in the forest, same trees every run \n",
    "predicted = cross_val_predict(model, X, actual, cv=5)  # Get predictions using CV, each part of CV is used once as a test set\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(actual, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be1c3c7",
   "metadata": {},
   "source": [
    "### Display Results -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb95ee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (with Cross-Validation):\n",
      "                Predicted\n",
      "Actual    | No Disease | Disease\n",
      "----------|------------|--------\n",
      "No Disease|      67     |     2\n",
      "Disease   |       9     |    22\n",
      "\n",
      "Metrics:\n",
      "True Negatives (TN): 67 - Correctly predicted no disease\n",
      "False Positives (FP): 2 - Incorrectly predicted disease\n",
      "False Negatives (FN): 9 - Incorrectly predicted no disease\n",
      "True Positives (TP): 22 - Correctly predicted disease\n"
     ]
    }
   ],
   "source": [
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix (with Cross-Validation):\")\n",
    "print(\"                Predicted\")\n",
    "print(\"Actual    | No Disease | Disease\")\n",
    "print(\"----------|------------|--------\")\n",
    "print(f\"No Disease|     {cm[0,0]:3d}     |   {cm[0,1]:3d}\")\n",
    "print(f\"Disease   |     {cm[1,0]:3d}     |   {cm[1,1]:3d}\")\n",
    "\n",
    "# Extract values from confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel() # cm.ravel() flattens the matrix into a 1D array\n",
    "\n",
    "# Show metrics derived from the confusion matrix\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"True Negatives (TN): {tn} - Correctly predicted no disease\")\n",
    "print(f\"False Positives (FP): {fp} - Incorrectly predicted disease\")\n",
    "print(f\"False Negatives (FN): {fn} - Incorrectly predicted no disease\")\n",
    "print(f\"True Positives (TP): {tp} - Correctly predicted disease\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e358c9",
   "metadata": {},
   "source": [
    "### Comparing Confusion Matrices Across Different Machine Learning Models\n",
    "\n",
    "Logistic Regression vs Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8471f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Create data again (same as above)\n",
    "np.random.seed(42)\n",
    "age = np.random.normal(55, 15, 100)\n",
    "cholesterol = np.random.normal(200, 50, 100)\n",
    "risk = (age - 50) / 20 + (cholesterol - 200) / 100\n",
    "actual = (risk > 0.5).astype(int)\n",
    "X = np.column_stack([age, cholesterol])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308d1eba",
   "metadata": {},
   "source": [
    "### Display Function -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34999cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract and display confusion matrix details for a model\n",
    "def show_results(cm, model_name):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4178db0",
   "metadata": {},
   "source": [
    "### Test Logistic Regression - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38790d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression:\n",
      "TN: 69, FP: 0, FN: 0, TP: 31\n",
      "Accuracy: 1.000 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and evaluate Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_predicted = cross_val_predict(lr_model, X, actual, cv=5)\n",
    "lr_cm = confusion_matrix(actual, lr_predicted)\n",
    "lr_accuracy = show_results(lr_cm, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74bc5d5",
   "metadata": {},
   "source": [
    "### Test Random Forest -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3dcbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest:\n",
      "TN: 67, FP: 2, FN: 9, TP: 22\n",
      "Accuracy: 0.890 (89.0%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and evaluate Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rf_predicted = cross_val_predict(rf_model, X, actual, cv=5) # 5-fold cv\n",
    "rf_cm = confusion_matrix(actual, rf_predicted)\n",
    "rf_accuracy = show_results(rf_cm, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe9f380",
   "metadata": {},
   "source": [
    "### Compare Both Models - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76a180a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison:\n",
      "Logistic Regression: 1.000\n",
      "Random Forest: 0.890\n",
      "Logistic Regression wins!\n"
     ]
    }
   ],
   "source": [
    "# Compare accuracies between the two models\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Logistic Regression: {lr_accuracy:.3f}\")\n",
    "print(f\"Random Forest: {rf_accuracy:.3f}\")\n",
    "\n",
    "# Determine which model wins\n",
    "if lr_accuracy > rf_accuracy:\n",
    "    print(\"Logistic Regression wins!\")\n",
    "else:\n",
    "    print(\"Random Forest wins!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e58d04",
   "metadata": {},
   "source": [
    "Confusion matrices help compare different ML models on the same dataset. By testing multiple algorithms, we can see which performs best and what types of errors each makes. This is especially important in medical applications where false negatives (missing disease) are more dangerous than false positives (false alarm). The confusion matrix guides us to choose the right model based on error patterns that matter most for our specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2f5bd",
   "metadata": {},
   "source": [
    "## Precision - \n",
    "\n",
    "When our model predicts \"Yes\" (has disease), how often is it correct?\n",
    "\n",
    "From the confusion matrix:\n",
    "\n",
    "True Positives (TP): Model correctly predicted disease.\n",
    "\n",
    "False Positives (FP): Model incorrectly predicted disease (but they were healthy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a218d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.917\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "# Display precision\n",
    "print(f\"Precision: {precision:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d816a0d",
   "metadata": {},
   "source": [
    "High precision means few false alarms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158394d9",
   "metadata": {},
   "source": [
    "### Sensitivity (True Positive Rate):\n",
    "\n",
    "The proportion of actual positive cases that were correctly identified\n",
    "\n",
    "Formula: TP / (TP + FN)\n",
    "\n",
    "Measures how well the model identifies people who actually have the condition\n",
    "\n",
    "### Recall - means we found most of the real cases.\n",
    "\n",
    "For example, in medical screening: it's better to catch every possible case, even if that includes a few false alarms.\n",
    "\n",
    "### Recall = True Positives / (True Positives + False Negatives)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef843b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall (Sensitivity): 0.710\n"
     ]
    }
   ],
   "source": [
    "# Recall is the same as sensitivity\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "# Display recall\n",
    "print(f\"Recall (Sensitivity): {recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728857b",
   "metadata": {},
   "source": [
    "### The Precision-Recall Trade-Off\n",
    "Improving one often lowers the other:\n",
    "\n",
    "High Precision â†’ fewer false positives (but might miss some real cases)\n",
    "\n",
    "High Recall â†’ catch more true positives (but might get more false alarms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b2493",
   "metadata": {},
   "source": [
    "### Specificity (True Negative Rate):\n",
    "\n",
    "The proportion of actual negative cases that were correctly identified\n",
    "\n",
    "Formula: TN / (TN + FP)\n",
    "\n",
    "Measures how well the model identifies people who don't have the condition\n",
    "\n",
    "### Calculating Sensitivity & Specificity - Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dfa9b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "TN: 67, FP: 2\n",
      "FN: 9, TP: 22\n",
      "\n",
      "Sensitivity = 22 / (22 + 9) = 0.710 (71.0%)\n",
      "Specificity = 67 / (67 + 2) = 0.971 (97.1%)\n",
      "\n",
      "Interpretation:\n",
      "- 71.0% of actual positive cases were correctly identified\n",
      "- 97.1% of actual negative cases were correctly identified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = np.array([[67, 2], [9, 22]])\n",
    "\n",
    "# Extract values from confusion confusion_matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate Sensitivity (True positive rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Calculate Specificity (True Negative rate)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(f\"TN: {tn}, FP: {fp}\")\n",
    "print(f\"FN: {fn}, TP: {tp}\")\n",
    "print(f\"\\nSensitivity = {tp} / ({tp} + {fn}) = {sensitivity:.3f} ({sensitivity*100:.1f}%)\")\n",
    "print(f\"Specificity = {tn} / ({tn} + {fp}) = {specificity:.3f} ({specificity*100:.1f}%)\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- {sensitivity*100:.1f}% of actual positive cases were correctly identified\")\n",
    "print(f\"- {specificity*100:.1f}% of actual negative cases were correctly identified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a05d48",
   "metadata": {},
   "source": [
    "## F1 Score â€“ The Balance Between Precision and Recall\n",
    "\n",
    "Used to measure a model's performance in a classification task. Provides a single score that balances the trade-off between precision and recall.\n",
    "\n",
    "F1 scores value ranged from 0 (worst performance) to 1 (best performance)\n",
    "\n",
    "### F1 Score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1754f62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.917\n",
      "Recall: 0.710\n",
      "F1 Score: 0.800\n"
     ]
    }
   ],
   "source": [
    "# Precision and Recall\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "# F1 Score\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Print Results\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485f87b6",
   "metadata": {},
   "source": [
    "### Multi-class Confusion Matrix\n",
    "\n",
    "What if there are more than two categories? \n",
    "\n",
    "A confusion matrix isn't limited to just two categories! If we were trying to predict if the disease is healthy, mild or severe, our confusion matrix would be 3x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c072b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Confusion Matrix:\n",
      "Predicted:  Healthy  Mild  Severe\n",
      "Actual:\n",
      "Healthy     45      5      0\n",
      "Mild         3     32      5\n",
      "Severe       0      2      8\n",
      "\n",
      "Healthy Sensitivity = 45 / (45 + 5) = 0.900 (90.0%)\n",
      "\n",
      "Mild Sensitivity = 32 / (32 + 8) = 0.800 (80.0%)\n",
      "\n",
      "Severe Sensitivity = 8 / (8 + 2) = 0.800 (80.0%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Multi-class example: Disease classification (Healthy, Mild, Severe)\n",
    "\n",
    "# Example: 3-class confusion matrix (Healthy=0, Mild=1, Severe=2)\n",
    "cm_multi = np.array([[45, 5, 0],    # Healthy patients\n",
    "                     [3, 32, 5],     # Mild patients  \n",
    "                     [0, 2, 8]])     # Severe patients\n",
    "\n",
    "print(\"Multi-class Confusion Matrix:\")\n",
    "print(\"Predicted:  Healthy  Mild  Severe\")\n",
    "print(\"Actual:\")\n",
    "print(f\"Healthy    {cm_multi[0,0]:3d}    {cm_multi[0,1]:3d}    {cm_multi[0,2]:3d}\")\n",
    "print(f\"Mild       {cm_multi[1,0]:3d}    {cm_multi[1,1]:3d}    {cm_multi[1,2]:3d}\")\n",
    "print(f\"Severe     {cm_multi[2,0]:3d}    {cm_multi[2,1]:3d}    {cm_multi[2,2]:3d}\")\n",
    "\n",
    "# Calculate sensitivity for each class (one-vs-rest approach)\n",
    "for i, class_name in enumerate(['Healthy', 'Mild', 'Severe']): # enumerate is used to get the index and the value of the class_name\n",
    "    # True positives for this class\n",
    "    tp = cm_multi[i, i]\n",
    "    # False negatives (sum of row minus true positives)\n",
    "    fn = np.sum(cm_multi[i, :]) - tp\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{class_name} Sensitivity = {tp} / ({tp} + {fn}) = {sensitivity:.3f} ({sensitivity*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d933586",
   "metadata": {},
   "source": [
    "Trade-off Relationship: Sensitivity and specificity often have an inverse relationship - improving one typically decreases the other\n",
    "\n",
    "\n",
    "A Confusion Matrix is a fundamental tool for understanding the performance of a classification model.\n",
    "\n",
    "--> It tells you what your algorithm did right.\n",
    "\n",
    "--> It tells you what your algorithm did wrong.\n",
    "\n",
    "--> It works for both binary (2x2) and multi-class (NxN) problems.\n",
    "\n",
    "--> It is the foundation for other important metrics like Sensitivity and Specificity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
